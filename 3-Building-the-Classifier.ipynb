{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Πρασινο αυκουι μες το πασχαλινο ποτήρι που έπιασε ο μιτσης #αισχος 🤣🤣🤣   @HARRIS_APOEL https://t.co/y9X7CmBEa5',\n",
       " '@HARRIS_APOEL @pirpoitis @vassrules Καμνουν ανακαινιση στα Περβολια φετος.',\n",
       " '@MUFCChristian Ελα συγγενη τζιαι εχουμε νεοτερα π το Νικολη.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "cg_sents = []\n",
    "smg_sents = []\n",
    "\n",
    "def remove_duplicate_punctuation(s): # sent_tokenize() gets confused when there's duplicate punctuation \n",
    "    return(re.sub(r'(\\.|\\?|!)\\1+', r'\\1', s))\n",
    "    \n",
    "with open('./Data/cg_twitter.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p] # sent_tokenize() doesn't consider a new line a new sentence so this is required.\n",
    "    for line in lines:\n",
    "        cg_sents += sent_tokenize(line)\n",
    "    \n",
    "with open('./Data/cg_fb.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        cg_sents += sent_tokenize(line)\n",
    "    \n",
    "with open('./Data/cg_other.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        cg_sents += sent_tokenize(line)\n",
    "\n",
    "with open('./Data/smg_twitter.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        smg_sents += sent_tokenize(line)\n",
    "    \n",
    "with open('./Data/smg_fb.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        smg_sents += sent_tokenize(line)\n",
    "    \n",
    "with open('./Data/smg_other.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        smg_sents += sent_tokenize(line)\n",
    "\n",
    "cg_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['πρασινο αυκουι μες το πασχαλινο ποτηρι που επιασε ο μιτσης',\n",
       " 'καμνουν ανακαινιση στα περβολια φετος',\n",
       " 'ελα συγγενη τζιαι εχουμε νεοτερα π το νικολη']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "from string import punctuation\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def get_clean_sent_el(sentence):\n",
    "    sentence = re.sub(r'^RT', '', sentence)\n",
    "    sentence = re.sub(r'\\&\\w*;', '', sentence)\n",
    "    sentence = re.sub(r'\\@\\w*', '', sentence)\n",
    "    sentence = re.sub(r'\\$\\w*', '', sentence)\n",
    "    sentence = re.sub(r'https?:\\/\\/.*\\/\\w*', '', sentence)\n",
    "    sentence = ''.join(c for c in sentence if c <= '\\uFFFF')\n",
    "    sentence = strip_accents(sentence)\n",
    "    sentence = re.sub(r'#\\w*', '', sentence)\n",
    "    tokens = WhitespaceTokenizer().tokenize(sentence)\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token == 'ο,τι' or token == 'ό,τι' or token == 'o,ti' or token == 'ó,ti':\n",
    "            new_tokens.append(token)\n",
    "        else:\n",
    "            new_tokens.append(token.translate(str.maketrans({key: None for key in punctuation})))\n",
    "    sentence =' '.join(new_tokens)\n",
    "    sentence = sentence.strip(' ') # performs lstrip() and rstrip()\n",
    "    return sentence.lower()\n",
    "\n",
    "cg_sents_clean = []\n",
    "smg_sents_clean = []\n",
    "\n",
    "for sent in cg_sents:\n",
    "    cg_sents_clean.append(get_clean_sent_el(sent))\n",
    "for sent in smg_sents:\n",
    "    smg_sents_clean.append(get_clean_sent_el(sent))\n",
    "\n",
    "# Remove empty strings left due to sentences ending up being only URLs then getting deleted on cleaning:\n",
    "cg_sents_clean = list(filter(None, cg_sents_clean))\n",
    "smg_sents_clean = list(filter(None, smg_sents_clean))\n",
    "cg_sents_clean[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['πρασινο',\n",
       "  'αυκουι',\n",
       "  'μες',\n",
       "  'το',\n",
       "  'πασχαλινο',\n",
       "  'ποτηρι',\n",
       "  'που',\n",
       "  'επιασε',\n",
       "  'ο',\n",
       "  'μιτσης'],\n",
       " ['καμνουν', 'ανακαινιση', 'στα', 'περβολια', 'φετος'],\n",
       " ['ελα', 'συγγενη', 'τζιαι', 'εχουμε', 'νεοτερα', 'π', 'το', 'νικολη']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg_sents_tokens = []\n",
    "smg_sents_tokens = []\n",
    "\n",
    "for sent in cg_sents_clean:\n",
    "    cg_sents_tokens.append(WhitespaceTokenizer().tokenize(sent))\n",
    "for sent in smg_sents_clean:\n",
    "    smg_sents_tokens.append(WhitespaceTokenizer().tokenize(sent))\n",
    "    \n",
    "cg_sents_tokens[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def get_word_ngrams(tokens, n):\n",
    "    ngrams_list = []\n",
    "    ngrams_list.append(list(ngrams(tokens, n)))\n",
    "    ngrams_flat_tuples = [ngram for ngram_list in ngrams_list for ngram in ngram_list]\n",
    "    format_string = '%s'\n",
    "    for i in range(1, n):\n",
    "        format_string += (' %s')\n",
    "    ngrams_list_flat = [format_string % ngram_tuple for ngram_tuple in ngrams_flat_tuples]\n",
    "    return ngrams_list_flat\n",
    "\n",
    "def get_char_ngrams(word, n):\n",
    "    ngrams_list = []\n",
    "    word = re.sub(r'ς', 'σ', word)\n",
    "    ngrams_list.append(list(ngrams(word, n, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')))\n",
    "    \n",
    "    # Removing redundant ngrams:\n",
    "    if (n > 2):\n",
    "        redundant_combinations = n - 2\n",
    "        ngrams_list = [ngram_list[redundant_combinations : -redundant_combinations] for ngram_list in ngrams_list]\n",
    "    \n",
    "    ngrams_flat_tuples = [ngram for ngram_list in ngrams_list for ngram in ngram_list]\n",
    "    format_string = ''\n",
    "    for i in range(0, n):\n",
    "        format_string += ('%s')\n",
    "    ngrams_list_flat = [format_string % ngram_tuple for ngram_tuple in ngrams_flat_tuples]\n",
    "    return ngrams_list_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char(α)': 3,\n",
       " 'char(ε)': 1,\n",
       " 'char(η)': 3,\n",
       " 'char(ι)': 2,\n",
       " 'char(ν)': 1,\n",
       " 'char(π)': 1,\n",
       " 'char(ρ)': 1,\n",
       " 'char(σ)': 1,\n",
       " 'char(τ)': 2,\n",
       " 'char(υ)': 1,\n",
       " 'char_bigram(_α)': 1,\n",
       " 'char_bigram(_ε)': 1,\n",
       " 'char_bigram(_η)': 1,\n",
       " 'char_bigram(_σ)': 1,\n",
       " 'char_bigram(αι)': 1,\n",
       " 'char_bigram(αρ)': 1,\n",
       " 'char_bigram(αυ)': 1,\n",
       " 'char_bigram(ει)': 1,\n",
       " 'char_bigram(η_)': 3,\n",
       " 'char_bigram(ι_)': 1,\n",
       " 'char_bigram(ιν)': 1,\n",
       " 'char_bigram(να)': 1,\n",
       " 'char_bigram(πα)': 1,\n",
       " 'char_bigram(ρτ)': 1,\n",
       " 'char_bigram(σπ)': 1,\n",
       " 'char_bigram(τη)': 2,\n",
       " 'char_bigram(υτ)': 1,\n",
       " 'char_quadrigram(_αυτ)': 1,\n",
       " 'char_quadrigram(_ειν)': 1,\n",
       " 'char_quadrigram(_σπα)': 1,\n",
       " 'char_quadrigram(αρτη)': 1,\n",
       " 'char_quadrigram(αυτη)': 1,\n",
       " 'char_quadrigram(εινα)': 1,\n",
       " 'char_quadrigram(ιναι)': 1,\n",
       " 'char_quadrigram(ναι_)': 1,\n",
       " 'char_quadrigram(παρτ)': 1,\n",
       " 'char_quadrigram(ρτη_)': 1,\n",
       " 'char_quadrigram(σπαρ)': 1,\n",
       " 'char_quadrigram(υτη_)': 1,\n",
       " 'char_trigram(_αυ)': 1,\n",
       " 'char_trigram(_ει)': 1,\n",
       " 'char_trigram(_η_)': 1,\n",
       " 'char_trigram(_σπ)': 1,\n",
       " 'char_trigram(αι_)': 1,\n",
       " 'char_trigram(αρτ)': 1,\n",
       " 'char_trigram(αυτ)': 1,\n",
       " 'char_trigram(ειν)': 1,\n",
       " 'char_trigram(ινα)': 1,\n",
       " 'char_trigram(ναι)': 1,\n",
       " 'char_trigram(παρ)': 1,\n",
       " 'char_trigram(ρτη)': 1,\n",
       " 'char_trigram(σπα)': 1,\n",
       " 'char_trigram(τη_)': 2,\n",
       " 'char_trigram(υτη)': 1,\n",
       " 'word(αυτη)': 1,\n",
       " 'word(ειναι)': 1,\n",
       " 'word(η)': 1,\n",
       " 'word(σπαρτη)': 1,\n",
       " 'word_bigram(αυτη ειναι)': 1,\n",
       " 'word_bigram(ειναι η)': 1,\n",
       " 'word_bigram(η σπαρτη)': 1,\n",
       " 'word_quadrigram(αυτη ειναι η σπαρτη)': 1,\n",
       " 'word_trigram(αυτη ειναι η)': 1,\n",
       " 'word_trigram(ειναι η σπαρτη)': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extractor\n",
    "def get_ngram_features(sentence_tokens):\n",
    "    features = {}\n",
    "    \n",
    "    # Word unigrams\n",
    "    ngrams = get_word_ngrams(sentence_tokens, 1)\n",
    "    for ngram in ngrams:\n",
    "        features[f'word({ngram})'] = features.get(f'word({ngram})', 0) + 1 # The second paramter to .get() is a default value if the key doesn't exist.\n",
    "    \n",
    "    # Word bigrams\n",
    "    ngrams = get_word_ngrams(sentence_tokens, 2)\n",
    "    for ngram in ngrams:\n",
    "        features[f'word_bigram({ngram})'] = features.get(f'word_bigram({ngram})', 0) + 1\n",
    "    \n",
    "    # Word trigrams\n",
    "    ngrams = get_word_ngrams(sentence_tokens, 3)\n",
    "    for ngram in ngrams:\n",
    "        features[f'word_trigram({ngram})'] = features.get(f'word_trigram({ngram})', 0) + 1\n",
    "    \n",
    "    # Word quadrigrams\n",
    "    ngrams = get_word_ngrams(sentence_tokens, 4)\n",
    "    for ngram in ngrams:\n",
    "        features[f'word_quadrigram({ngram})'] = features.get(f'word_quadrigram({ngram})', 0) + 1\n",
    "    \n",
    "    # Char unigrams\n",
    "    for word in sentence_tokens:\n",
    "        ngrams = get_word_ngrams(word, 1)\n",
    "        for ngram in ngrams:\n",
    "            features[f'char({ngram})'] = features.get(f'char({ngram})', 0) + 1\n",
    "    \n",
    "    # Char bigrams\n",
    "    for word in sentence_tokens:\n",
    "        ngrams = get_char_ngrams(word, 2)\n",
    "        for ngram in ngrams:\n",
    "            features[f'char_bigram({ngram})'] = features.get(f'char_bigram({ngram})', 0) + 1\n",
    "    \n",
    "    # Char trigrams\n",
    "    for word in sentence_tokens:\n",
    "        ngrams = get_char_ngrams(word, 3)\n",
    "        for ngram in ngrams:\n",
    "            features[f'char_trigram({ngram})'] = features.get(f'char_trigram({ngram})', 0) + 1\n",
    "    \n",
    "    # Char quadrigrams\n",
    "    for word in sentence_tokens:\n",
    "        ngrams = get_char_ngrams(word, 4)\n",
    "        for ngram in ngrams:\n",
    "            features[f'char_quadrigram({ngram})'] = features.get(f'char_quadrigram({ngram})', 0) + 1\n",
    "    \n",
    "    return features\n",
    "\n",
    "get_ngram_features(['αυτη', 'ειναι', 'η', 'σπαρτη'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Labeling the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['πρασινο',\n",
       "  'αυκουι',\n",
       "  'μες',\n",
       "  'το',\n",
       "  'πασχαλινο',\n",
       "  'ποτηρι',\n",
       "  'που',\n",
       "  'επιασε',\n",
       "  'ο',\n",
       "  'μιτσης'],\n",
       " 'cg')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cg_sents_features_labeled = [(get_ngram_features(word), 'cg') for word in cg_sents_tokens]\n",
    "# smg_sents_features_labeled = [(get_ngram_features(word), 'smg') for word in smg_sents_tokens]\n",
    "\n",
    "# all_sents_features_labeled = cg_sents_features_labeled + smg_sents_features_labeled\n",
    "# all_sents_features_labeled[0]\n",
    "\n",
    "all_sents_labeled = ([(sentence, 'cg') for sentence in cg_sents_tokens] + [(sentence, 'smg') for sentence in smg_sents_tokens])\n",
    "all_sents_labeled[0]                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Splitting corpus into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET\t SENTENCES\n",
      "All\t 153\n",
      "Training 122\n",
      "Testing\t 31\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.classify import apply_features\n",
    "\n",
    "random.shuffle(all_sents_labeled)\n",
    "\n",
    "NO_ALL_SENTENCES = len(all_sents_labeled)\n",
    "NO_TRAIN_SENTENCES = round(NO_ALL_SENTENCES * .8)\n",
    "\n",
    "print('DATASET\\t', 'SENTENCES')\n",
    "print('All\\t', NO_ALL_SENTENCES)\n",
    "print('Training', NO_TRAIN_SENTENCES)\n",
    "print('Testing\\t', NO_ALL_SENTENCES - NO_TRAIN_SENTENCES)\n",
    "\n",
    "train_set = apply_features(get_ngram_features, all_sents_labeled[:NO_TRAIN_SENTENCES])\n",
    "test_set = apply_features(get_ngram_features, all_sents_labeled[NO_TRAIN_SENTENCES:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hzsab\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cg', 'smg']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hzsab\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import accuracy\n",
    "\n",
    "round(accuracy(classifier, test_set), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "   char_quadrigram(εινα) = 1                 smg : cg     =     11.9 : 1.0\n",
      "       char_trigram(ρωτ) = 1                 smg : cg     =     11.9 : 1.0\n",
      "                 char(η) = 4                 smg : cg     =     11.1 : 1.0\n",
      "   char_quadrigram(ναι_) = 1                 smg : cg     =     10.1 : 1.0\n",
      "                 char(χ) = 2                 smg : cg     =     10.1 : 1.0\n",
      "       char_trigram(ναι) = 1                 smg : cg     =     10.0 : 1.0\n",
      "         char_bigram(ωτ) = 1                 smg : cg     =      8.3 : 1.0\n",
      "   char_quadrigram(σαι_) = 1                 smg : cg     =      8.3 : 1.0\n",
      "       char_trigram(ελ_) = 1                 smg : cg     =      8.3 : 1.0\n",
      "       char_trigram(εχε) = 1                 smg : cg     =      8.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
